{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "# import the necessary packages\n",
    "from imutils.video import VideoStream\n",
    "from imutils.video import FPS\n",
    "import numpy as np\n",
    "import argparse\n",
    "import imutils\n",
    "import pickle\n",
    "import time\n",
    "import cv2\n",
    "import os\n",
    "import embedding\n",
    "import resize\n",
    "import NN_classifier as nn\n",
    "\n",
    "# construct the argument parser and parse the arguments\n",
    "\n",
    "# ap = argparse.ArgumentParser()\n",
    "# ap.add_argument(\"-d\", \"--detector\", required=True,\n",
    "#     help=\"path to OpenCV's deep learning face detector\")\n",
    "# ap.add_argument(\"-m\", \"--embedding-model\", required=True,\n",
    "#     help=\"path to OpenCV's deep learning face embedding model\")\n",
    "# ap.add_argument(\"-r\", \"--recognizer\", required=True,\n",
    "#     help=\"path to model trained to recognize faces\")\n",
    "# ap.add_argument(\"-l\", \"--le\", required=True,\n",
    "#     help=\"path to label encoder\")\n",
    "# ap.add_argument(\"-c\", \"--confidence\", type=float, default=0.5,\n",
    "#     help=\"minimum probability to filter weak detections\")\n",
    "# args = vars(ap.parse_args())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {}\n",
    "args[\"detector\"] = \"./model/face_detection_model\"\n",
    "args[\"embedding_model\"] = \"./model/20180402-114759.pb\"\n",
    "args[\"recognizer\"] = \"./model/NN_model.json\"\n",
    "args[\"normalization_model\"] = \"./model/sc.pkl\"\n",
    "args[\"le\"] = \"./model/le.pickle\"\n",
    "args[\"confidence\"] = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading face detector...\n",
      "[INFO] loading face recognizer...\n"
     ]
    }
   ],
   "source": [
    "# load our serialized face detector from disk\n",
    "\n",
    "print(\"[INFO] loading face detector...\")\n",
    "protoPath = os.path.sep.join([args[\"detector\"], \"deploy.prototxt\"])\n",
    "modelPath = os.path.sep.join([args[\"detector\"],\n",
    "    \"res10_300x300_ssd_iter_140000.caffemodel\"])\n",
    "detector = cv2.dnn.readNetFromCaffe(protoPath, modelPath)\n",
    "\n",
    "# load our serialized face embedding model from disk\n",
    "print(\"[INFO] loading face recognizer...\")\n",
    "# embedder = cv2.dnn.readNetFromTorch(args[\"embedding_model\"])\n",
    "# load the actual face recognition model along with the label encoder\n",
    "# recognizer = pickle.loads(open(args[\"recognizer\"], \"rb\").read())\n",
    "sc = pickle.load(open(args[\"normalization_model\"],'rb'))\n",
    "recognizer = nn.TFNN.load(args[\"recognizer\"])\n",
    "le = pickle.loads(open(args[\"le\"], \"rb\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import facenet\n",
    "# import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model filename: ./model/20180402-114759.pb\n",
      "WARNING:tensorflow:From E:\\studyslides\\DS 5500\\project\\code\\Webcam\\facenet.py:383: FastGFile.__init__ (from tensorflow.python.platform.gfile) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.gfile.GFile.\n"
     ]
    }
   ],
   "source": [
    "# facenet.load_model(args[\"embedding_model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tf.Graph().as_default():\n",
    "#     with tf.compat.v1.Session() as sess:\n",
    "#         np.random.seed(seed=666)\n",
    "#         # Load the model\n",
    "#         print('Loading feature extraction model')\n",
    "#         facenet.load_model(args[\"embedding_model\"])\n",
    "\n",
    "        \n",
    "#         # initialize the video stream, then allow the camera sensor to warm up\n",
    "#         print(\"[INFO] starting video stream...\")\n",
    "#         vs = VideoStream(src=0).start()\n",
    "#         time.sleep(2.0)\n",
    "\n",
    "#         # start the FPS throughput estimator\n",
    "#         fps = FPS().start()\n",
    "        \n",
    "        \n",
    "#         # loop over frames from the video file stream\n",
    "#         while True:\n",
    "#             # grab the frame from the threaded video stream\n",
    "#             frame = vs.read()\n",
    "\n",
    "#             # resize the frame to have a width of 600 pixels (while\n",
    "#             # maintaining the aspect ratio), and then grab the image\n",
    "#             # dimensions\n",
    "#             frame = imutils.resize(frame, width=600)\n",
    "#             (h, w) = frame.shape[:2]\n",
    "\n",
    "#             # construct a blob from the image\n",
    "#             imageBlob = cv2.dnn.blobFromImage(\n",
    "#                 cv2.resize(frame, (300, 300)), 1.0, (300, 300),\n",
    "#                 (104.0, 177.0, 123.0), swapRB=False, crop=False)\n",
    "\n",
    "#             # apply OpenCV's deep learning-based face detector to localize\n",
    "#             # faces in the input image\n",
    "#             detector.setInput(imageBlob)\n",
    "#             detections = detector.forward()\n",
    "\n",
    "#             # loop over the detections\n",
    "#             for i in range(0, detections.shape[2]):\n",
    "#                 # extract the confidence (i.e., probability) associated with\n",
    "#                 # the prediction\n",
    "#                 confidence = detections[0, 0, i, 2]\n",
    "\n",
    "#                 # filter out weak detections\n",
    "#                 if confidence > args[\"confidence\"]:\n",
    "#                     # compute the (x, y)-coordinates of the bounding box for\n",
    "#                     # the face\n",
    "#                     box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "#                     (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "\n",
    "#                     # extract the face ROI\n",
    "#                     face = frame[startY:endY, startX:endX]\n",
    "#                     (fH, fW) = face.shape[:2]\n",
    "\n",
    "#                     # ensure the face width and height are sufficiently large\n",
    "#                     if fW < 20 or fH < 20:\n",
    "#                         continue\n",
    "\n",
    "\n",
    "#                     img = resize.resize_addframe(face, 160, 160)\n",
    "            \n",
    "                    \n",
    "#                     # Get input and output tensors\n",
    "#                     images_placeholder = tf.compat.v1.get_default_graph().get_tensor_by_name(\"input:0\")\n",
    "#                     embeddings = tf.compat.v1.get_default_graph().get_tensor_by_name(\"embeddings:0\")\n",
    "#                     phase_train_placeholder = tf.compat.v1.get_default_graph().get_tensor_by_name(\"phase_train:0\")\n",
    "#                     embedding_size = embeddings.get_shape()[1]\n",
    "\n",
    "#                     # Run forward pass to calculate embeddings\n",
    "#                     print('Calculating features for images')\n",
    "#                     emb_array = np.zeros((1, embedding_size))\n",
    "#                     images = facenet.load_data(img, False, False, 160)\n",
    "#                     feed_dict = { images_placeholder:images, phase_train_placeholder:False }\n",
    "#                     emb_array = sess.run(embeddings, feed_dict=feed_dict)\n",
    "#                     vec = emb_array\n",
    "            \n",
    "            \n",
    "# #                     vec = embedding.main(img, args[\"embedding_model\"], 160)\n",
    "\n",
    "#                     # perform classification to recognize the face\n",
    "#                     proba, pred_label = recognizer.predict_label(vec)\n",
    "#                     print(pred_label, proba)\n",
    "#                     name = le.classes_[int(pred_label)]\n",
    "\n",
    "#                     # draw the bounding box of the face along with the\n",
    "#                     # associated probability\n",
    "#                     text = \"{}: {:.2f}%\".format(name, float(proba) * 100)\n",
    "#                     y = startY - 10 if startY - 10 > 10 else startY + 10\n",
    "#                     cv2.rectangle(frame, (startX, startY), (endX, endY),\n",
    "#                         (0, 0, 255), 2)\n",
    "#                     cv2.putText(frame, text, (startX, y),\n",
    "#                         cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 0, 255), 2)\n",
    "\n",
    "#             # update the FPS counter\n",
    "#             fps.update()\n",
    "\n",
    "#             # show the output frame\n",
    "#             cv2.imshow(\"Frame\", frame)\n",
    "#             key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "#             # if the `q` key was pressed, break from the loop\n",
    "#             if key == ord(\"q\"):\n",
    "#                 break\n",
    "\n",
    "#         # stop the timer and display FPS information\n",
    "#         fps.stop()\n",
    "#         print(\"[INFO] elasped time: {:.2f}\".format(fps.elapsed()))\n",
    "#         print(\"[INFO] approx. FPS: {:.2f}\".format(fps.fps()))\n",
    "\n",
    "#         # do a bit of cleanup\n",
    "#         cv2.destroyAllWindows()\n",
    "#         vs.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] starting video stream...\n",
      "embedding start:  2020-11-20 16:58:02\n",
      "all start: 2020-11-20 16:58:02\n",
      "Model filename: ./model/20180402-114759.pb\n",
      "Calculating features for images\n",
      "INFO:tensorflow:Restoring parameters from ./tf.model\n",
      "embedding start:  2020-11-20 16:58:06\n",
      "all start: 2020-11-20 16:58:06\n",
      "Model filename: ./model/20180402-114759.pb\n",
      "Calculating features for images\n",
      "INFO:tensorflow:Restoring parameters from ./tf.model\n",
      "embedding start:  2020-11-20 16:58:10\n",
      "all start: 2020-11-20 16:58:10\n",
      "Model filename: ./model/20180402-114759.pb\n",
      "Calculating features for images\n",
      "INFO:tensorflow:Restoring parameters from ./tf.model\n",
      "embedding start:  2020-11-20 16:58:14\n",
      "all start: 2020-11-20 16:58:14\n",
      "Model filename: ./model/20180402-114759.pb\n",
      "Calculating features for images\n",
      "INFO:tensorflow:Restoring parameters from ./tf.model\n",
      "[INFO] elasped time: 16.01\n",
      "[INFO] approx. FPS: 0.25\n"
     ]
    }
   ],
   "source": [
    "# initialize the video stream, then allow the camera sensor to warm up\n",
    "print(\"[INFO] starting video stream...\")\n",
    "vs = VideoStream(src=0).start()\n",
    "time.sleep(2.0)\n",
    "\n",
    "# start the FPS throughput estimator\n",
    "fps = FPS().start()\n",
    "\n",
    "\n",
    "# loop over frames from the video file stream\n",
    "cnt = 0\n",
    "while True:\n",
    "    cnt += 1\n",
    "    # grab the frame from the threaded video stream\n",
    "    frame = vs.read()\n",
    "\n",
    "    # resize the frame to have a width of 600 pixels (while\n",
    "    # maintaining the aspect ratio), and then grab the image\n",
    "    # dimensions\n",
    "    frame = imutils.resize(frame, width=600)\n",
    "    (h, w) = frame.shape[:2]\n",
    "\n",
    "    # construct a blob from the image\n",
    "    imageBlob = cv2.dnn.blobFromImage(\n",
    "        cv2.resize(frame, (300, 300)), 1.0, (300, 300),\n",
    "        (104.0, 177.0, 123.0), swapRB=False, crop=False)\n",
    "\n",
    "    # apply OpenCV's deep learning-based face detector to localize\n",
    "    # faces in the input image\n",
    "    detector.setInput(imageBlob)\n",
    "    detections = detector.forward()\n",
    "\n",
    "    # loop over the detections\n",
    "    for i in range(0, detections.shape[2]):\n",
    "        # extract the confidence (i.e., probability) associated with\n",
    "        # the prediction\n",
    "        confidence = detections[0, 0, i, 2]\n",
    "\n",
    "        # filter out weak detections\n",
    "        if confidence > args[\"confidence\"]:\n",
    "            # compute the (x, y)-coordinates of the bounding box for\n",
    "            # the face\n",
    "            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "            (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "\n",
    "            # extract the face ROI\n",
    "            face = frame[startY:endY, startX:endX]\n",
    "            (fH, fW) = face.shape[:2]\n",
    "\n",
    "            # ensure the face width and height are sufficiently large\n",
    "            if fW < 20 or fH < 20:\n",
    "                continue\n",
    "\n",
    "                \n",
    "                \n",
    "        \n",
    "            # construct a blob for the face ROI, then pass the blob\n",
    "            # through our face embedding model to obtain the 128-d\n",
    "            # quantification of the face\n",
    "#             faceBlob = cv2.dnn.blobFromImage(face, 1.0 / 255,\n",
    "#                 (160, 160), (0, 0, 0), swapRB=True, crop=False)\n",
    "#             embedder.setInput(faceBlob)\n",
    "#             vec = embedder.forward()\n",
    "            \n",
    "            \n",
    "            img = resize.resize_addframe(face, 160, 160)\n",
    "            cv2.imwrite(\"./image/\"+str(cnt)+\".jpg\", img)\n",
    "            print(\"embedding start: \", time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()))\n",
    "            vec = embedding.main(img, args[\"embedding_model\"], 160)\n",
    "\n",
    "            # perform classification to recognize the face\n",
    "#             print(\"embedding end, nn start: \", time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()))\n",
    "            vec_nor = nn.get_normalized_data('Classifiy',vec,sc = sc)\n",
    "            proba, pred_label = recognizer.predict_label(vec_nor)\n",
    "#             print(pred_label, proba)\n",
    "            name = le.classes_[int(pred_label)]\n",
    "#             print(\"nn end: \", time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()))\n",
    "\n",
    "            # draw the bounding box of the face along with the\n",
    "            # associated probability\n",
    "            text = \"{}: {:.2f}%\".format(name, float(proba) * 100)\n",
    "            y = startY - 10 if startY - 10 > 10 else startY + 10\n",
    "            cv2.rectangle(frame, (startX, startY), (endX, endY),\n",
    "                (0, 0, 255), 2)\n",
    "            cv2.putText(frame, text, (startX, y),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 0, 255), 2)\n",
    "\n",
    "    # update the FPS counter\n",
    "    fps.update()\n",
    "\n",
    "    # show the output frame\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "    # if the `q` key was pressed, break from the loop\n",
    "    if key == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "# stop the timer and display FPS information\n",
    "fps.stop()\n",
    "print(\"[INFO] elasped time: {:.2f}\".format(fps.elapsed()))\n",
    "print(\"[INFO] approx. FPS: {:.2f}\".format(fps.fps()))\n",
    "\n",
    "# do a bit of cleanup\n",
    "cv2.destroyAllWindows()\n",
    "vs.stop()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[INFO] starting video stream...\n",
    "embedding start:  2020-11-16 20:54:03\n",
    "all start: 2020-11-16 20:54:03\n",
    "LOAD FACENET start: 2020-11-16 20:54:03\n",
    "Model filename: ./model/20180402-114759.pb\n",
    "LOAD FACENET END, tensor start: 2020-11-16 20:54:04\n",
    "tensor end, embedding start: 2020-11-16 20:54:04\n",
    "Calculating features for images\n",
    "embed1 end, embedding start: 2020-11-16 20:54:04\n",
    "embed2 end, embedding start: 2020-11-16 20:54:04\n",
    "embed3 end, embedding start: 2020-11-16 20:54:04\n",
    "embedding end: 2020-11-16 20:54:07\n",
    "embedding end, nn start:  2020-11-16 20:54:07\n",
    "INFO:tensorflow:Restoring parameters from ./tf.model\n",
    "[0] [0.9636666]\n",
    "nn end:  2020-11-16 20:54:07"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
